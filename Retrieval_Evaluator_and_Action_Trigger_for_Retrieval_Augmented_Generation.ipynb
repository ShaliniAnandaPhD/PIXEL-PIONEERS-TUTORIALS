{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv4J5MoLd17iJUIoaQchHo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/PIXEL-PIONEERS-TUTORIALS/blob/main/Retrieval_Evaluator_and_Action_Trigger_for_Retrieval_Augmented_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The trigger action RAG (Retrieval-Augmented Generation) builds on top of regular RAG by introducing a retrieval evaluator and a mechanism to trigger different actions based on the relevance scores of the retrieved documents. This extension aims to improve the quality and relevance of the generated outputs by selectively using the retrieved documents based on their relevance to the input query.**\n",
        "\n",
        "Here's how the trigger action RAG extends the regular RAG approach:\n",
        "\n",
        "1. **Retrieval Evaluator:**\n",
        "   - In regular RAG, the retrieved documents are directly used for generating the output without explicitly evaluating their relevance to the input query.\n",
        "   - The trigger action RAG introduces a retrieval evaluator component, which is typically a pre-trained language model (e.g., T5) fine-tuned to assess the relevance of the retrieved documents to the input query.\n",
        "   - The retrieval evaluator assigns relevance scores to each retrieved document, indicating how well they match the input query.\n",
        "\n",
        "2. **Triggered Actions:**\n",
        "   - Regular RAG simply uses the retrieved documents as additional context for generating the output, regardless of their relevance.\n",
        "   - The trigger action RAG defines different actions based on the relevance scores assigned by the retrieval evaluator.\n",
        "   - The actions can be categorized as \"Correct\", \"Incorrect\", or \"Ambiguous\", depending on the relevance scores and predefined thresholds.\n",
        "   - The \"Correct\" action is triggered when the relevance scores are high, indicating that the retrieved documents are highly relevant to the input query. In this case, the selected documents are used for generating the output.\n",
        "   - The \"Incorrect\" action is triggered when the relevance scores are low, suggesting that the retrieved documents are not relevant to the input query. In this case, the retrieved documents are discarded, and the model may rely on its own knowledge or generate a response without using the retrieved information.\n",
        "   - The \"Ambiguous\" action is triggered when the relevance scores are mixed or fall between the thresholds. In this case, the model may use a combination of the retrieved documents and its own knowledge to generate the output.\n",
        "\n",
        "3. **Document Selection:**\n",
        "   - Regular RAG uses all the retrieved documents for generating the output.\n",
        "   - The trigger action RAG selectively uses the retrieved documents based on the triggered action and the relevance scores.\n",
        "   - For the \"Correct\" action, only the documents with high relevance scores above a certain threshold are selected.\n",
        "   - For the \"Incorrect\" action, no documents are selected, and the model generates the output without using the retrieved information.\n",
        "   - For the \"Ambiguous\" action, all the retrieved documents may be used, or a subset of documents with relevance scores above a certain threshold may be selected.\n",
        "\n",
        "By incorporating the retrieval evaluator and triggered actions, the trigger action RAG aims to improve the quality and relevance of the generated outputs. It allows the model to selectively use the retrieved documents based on their relevance to the input query, potentially reducing the reliance on irrelevant or noisy information.\n",
        "\n",
        "The trigger action RAG can be seen as an extension of regular RAG that adds an additional layer of relevance evaluation and decision-making based on the retrieved documents. This extension can help in scenarios where the retrieved documents may not always be relevant or useful for generating the desired output, and the model needs to adapt its generation strategy accordingly."
      ],
      "metadata": {
        "id": "nsVZPZKB2oqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Relevance Evaluation and Action Triggering with T5\n",
        "\n",
        "### Description:\n",
        "\n",
        "This code demonstrates a comprehensive approach to evaluating the relevance of retrieved documents to a given question and subsequently triggering appropriate actions based on the evaluated relevance scores. It leverages the T5 (Text-to-Text Transfer Transformer) model and tokenizer for the evaluation process. The code consists of two main functions:\n",
        "\n",
        "1. **`evaluate_retrieval`**: This function takes an input question, a list of retrieved documents, a pre-trained T5 model, and its tokenizer to evaluate the relevance of each document to the question. It does so by concatenating the question and each document, feeding this input to the T5 model, and interpreting the model's output as a relevance score (1.0 for relevant, 0.0 for not relevant, and 0.5 for ambiguous cases).\n",
        "\n",
        "2. **`trigger_action`**: Based on the relevance scores obtained from `evaluate_retrieval`, this function decides on the appropriate action to take. It uses two thresholds: an upper threshold to trigger a \"Correct\" action (indicating high relevance) and a lower threshold for an \"Incorrect\" action (indicating low relevance). Documents scoring above the upper threshold are selected for further processing, while those below the lower threshold are discarded. Documents with scores in between are treated as ambiguous, potentially requiring additional review.\n",
        "\n",
        "The example usage illustrates how to use these functions with a sample question and a set of retrieved documents. It shows how to obtain relevance scores and determine the next steps based on these scores, demonstrating a practical application of machine learning in information retrieval tasks."
      ],
      "metadata": {
        "id": "iMvMQSgHw09h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Paper for Retrieval Evaluator and Action Trigger for RAG](https://arxiv.org/html/2401.15884v2)\n",
        "\n"
      ],
      "metadata": {
        "id": "H0xrwSBtyeYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example usage of retrieval evaluation and action trigger"
      ],
      "metadata": {
        "id": "GSzm_UFXzs9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hj_d5fEpSGB",
        "outputId": "b836f7b0-3acc-49e1-95ca-80b48df9a2c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggered Action: Ambiguous\n",
            "Selected Documents: ['Henry Feilden was a Conservative Party politician.', 'Henry Master Feilden was educated at Eton College and served in the Second Boer War.', 'The Feilden Baronetcy, of Feniscowles in Lancashire, was a title in the Baronetage of England.']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer to use as the retrieval evaluator\n",
        "model_name = 't5-large'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "evaluator_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def evaluate_retrieval(question, retrieved_docs, evaluator_model, tokenizer):\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for doc in retrieved_docs:\n",
        "        # Concatenate question and document as input\n",
        "        input_text = f\"Question: {question} Document: {doc} Relevant:\"\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "        # Get relevance score from evaluator model\n",
        "        with torch.no_grad():\n",
        "            outputs = evaluator_model.generate(input_ids, max_length=5)\n",
        "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if decoded_output.lower() == 'true':\n",
        "            score = 1.0\n",
        "        elif decoded_output.lower() == 'false':\n",
        "            score = 0.0\n",
        "        else:\n",
        "            score = 0.5  # Set a default score for unexpected output\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def trigger_action(question, retrieved_docs, scores, upper_threshold=0.8, lower_threshold=0.2):\n",
        "\n",
        "    if max(scores) > upper_threshold:\n",
        "        # Trigger Correct action\n",
        "        action = 'Correct'\n",
        "        # Select documents with score > upper_threshold\n",
        "        selected_docs = [doc for doc, score in zip(retrieved_docs, scores) if score > upper_threshold]\n",
        "    elif max(scores) < lower_threshold:\n",
        "        # Trigger Incorrect action\n",
        "        action = 'Incorrect'\n",
        "        selected_docs = [] # Discard retrieved docs\n",
        "    else:\n",
        "        # Trigger Ambiguous action\n",
        "        action = 'Ambiguous'\n",
        "        selected_docs = retrieved_docs # Use all retrieved docs\n",
        "\n",
        "    return action, selected_docs\n",
        "\n",
        "# Example usage\n",
        "question = \"What is Henry Feilden's occupation?\"\n",
        "retrieved_docs = [\n",
        "    \"Henry Feilden was a Conservative Party politician.\",\n",
        "    \"Henry Master Feilden was educated at Eton College and served in the Second Boer War.\",\n",
        "    \"The Feilden Baronetcy, of Feniscowles in Lancashire, was a title in the Baronetage of England.\"\n",
        "]\n",
        "\n",
        "# Get relevance scores from retrieval evaluator\n",
        "scores = evaluate_retrieval(question, retrieved_docs, evaluator_model, tokenizer)\n",
        "\n",
        "# Trigger appropriate action based on scores\n",
        "action, selected_docs = trigger_action(question, retrieved_docs, scores)\n",
        "\n",
        "print(f\"Triggered Action: {action}\")\n",
        "print(f\"Selected Documents: {selected_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fact Verification"
      ],
      "metadata": {
        "id": "cV-FOLGbwHfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claim = \"The Eiffel Tower is located in Rome, Italy.\"\n",
        "retrieved_docs = [\n",
        "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
        "    \"Rome is the capital city of Italy and a special comune.\",\n",
        "    \"The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy.\"\n",
        "]\n",
        "\n",
        "# Get relevance scores from retrieval evaluator\n",
        "scores = evaluate_retrieval(claim, retrieved_docs, evaluator_model, tokenizer)\n",
        "\n",
        "# Trigger appropriate action based on scores\n",
        "action, selected_docs = trigger_action(claim, retrieved_docs, scores)\n",
        "\n",
        "print(f\"Triggered Action: {action}\")\n",
        "print(f\"Selected Documents: {selected_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3MHVjYwwKX9",
        "outputId": "ee61e8c3-ab9b-4058-d75a-6f138e1b5aea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggered Action: Ambiguous\n",
            "Selected Documents: ['The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.', 'Rome is the capital city of Italy and a special comune.', 'The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question Generation"
      ],
      "metadata": {
        "id": "4ZvgnddjwPVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = \"Ernest Hemingway\"\n",
        "retrieved_docs = [\n",
        "    \"Ernest Hemingway was an American novelist, short-story writer, journalist, and sportsman.\",\n",
        "    \"Hemingway's distinctive writing style is characterized by economy and understatement.\",\n",
        "    \"Hemingway published seven novels, six short-story collections, and two nonfiction works during his lifetime.\"\n",
        "]\n",
        "\n",
        "# Get relevance scores from retrieval evaluator\n",
        "scores = evaluate_retrieval(answer, retrieved_docs, evaluator_model, tokenizer)\n",
        "\n",
        "# Trigger appropriate action based on scores\n",
        "action, selected_docs = trigger_action(answer, retrieved_docs, scores)\n",
        "\n",
        "print(f\"Triggered Action: {action}\")\n",
        "print(f\"Selected Documents: {selected_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcn_VYyYwSon",
        "outputId": "14a0eeb2-7dd7-40c2-cdaa-fdab69dd92e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggered Action: Ambiguous\n",
            "Selected Documents: ['Ernest Hemingway was an American novelist, short-story writer, journalist, and sportsman.', \"Hemingway's distinctive writing style is characterized by economy and understatement.\", 'Hemingway published seven novels, six short-story collections, and two nonfiction works during his lifetime.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstractive Summarization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e3YP8dKowXGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"The Mona Lisa is a 16th-century portrait painted in oil on a poplar panel by Leonardo da Vinci. It is probably the world's most famous painting, and is one of the most parodied works of art. The painting is a portrait of Lisa Gherardini, the wife of Francesco del Giocondo, and is in oil on a white Lombardy poplar panel. It had been believed to have been painted between 1503 and 1506; however, Leonardo may have continued working on it as late as 1517. It was acquired by King Francis I of France and is now the property of the French Republic. It has been on permanent display at the Louvre in Paris since 1797.\"\n",
        "retrieved_docs = [\n",
        "    \"The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci.\",\n",
        "    \"The painting is a portrait of Lisa Gherardini, the wife of Francesco del Giocondo.\",\n",
        "    \"The Mona Lisa is one of the most valuable paintings in the world.\"\n",
        "]\n",
        "\n",
        "# Get relevance scores from retrieval evaluator\n",
        "scores = evaluate_retrieval(document, retrieved_docs, evaluator_model, tokenizer)\n",
        "\n",
        "# Trigger appropriate action based on scores\n",
        "action, selected_docs = trigger_action(document, retrieved_docs, scores)\n",
        "\n",
        "print(f\"Triggered Action: {action}\")\n",
        "print(f\"Selected Documents: {selected_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2W9AygAwWZh",
        "outputId": "9239c3b6-8d54-4188-aaee-cdd66d14c04a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggered Action: Ambiguous\n",
            "Selected Documents: ['The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci.', 'The painting is a portrait of Lisa Gherardini, the wife of Francesco del Giocondo.', 'The Mona Lisa is one of the most valuable paintings in the world.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigger - Ambiguous, Correct vs Incorrect"
      ],
      "metadata": {
        "id": "4Fd4_32_z28A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the same input document about \"The Starry Night\" painting and provide a mix of relevant and irrelevant retrieved documents. We manually set the scores to demonstrate all three triggered actions:\n",
        "\n",
        "The first retrieved document is highly relevant, so it gets a score of 0.9.\n",
        "The second retrieved document is irrelevant, so it gets a score of 0.1.\n",
        "The third retrieved document is somewhat relevant, so it gets a score of 0.6.\n",
        "Based on these scores and the defined thresholds (upper_threshold=0.8 and lower_threshold=0.2), the triggered action will be \"Ambiguous\" because:\n",
        "\n",
        "The maximum score (0.9) is greater than the upper_threshold, triggering the \"Correct\" action.\n",
        "However, there is also a score (0.1) that is lower than the lower_threshold, which would trigger the \"Incorrect\" action.\n",
        "Since there are conflicting scores, the \"Ambiguous\" action is triggered as a fallback.\n",
        "The selected documents will include all the retrieved documents since the action is \"Ambiguous\".\n",
        "\n",
        "When you run this code, it will output the triggered action as \"Ambiguous\" and the selected documents, demonstrating how the code handles a mix of relevant and irrelevant retrieved documents.\n"
      ],
      "metadata": {
        "id": "sUAmsNX41RG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer to use as the retrieval evaluator\n",
        "model_name = 't5-large'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "evaluator_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def evaluate_retrieval(document, retrieved_docs, evaluator_model, tokenizer):\n",
        "    '''\n",
        "    Evaluate the relevance of retrieved documents to the input document.\n",
        "\n",
        "    Args:\n",
        "        document (str): The input document\n",
        "        retrieved_docs (list): List of retrieved documents\n",
        "        evaluator_model: The T5 model used as the retrieval evaluator\n",
        "        tokenizer: The T5 tokenizer\n",
        "\n",
        "    Returns:\n",
        "        scores (list): Relevance scores between 0 and 1 for each retrieved doc\n",
        "    '''\n",
        "    scores = []\n",
        "\n",
        "    for doc in retrieved_docs:\n",
        "        # Concatenate document and retrieved doc as input\n",
        "        input_text = f\"Document: {document} Retrieved: {doc} Relevant:\"\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "        # Get relevance score from evaluator model\n",
        "        with torch.no_grad():\n",
        "            outputs = evaluator_model.generate(input_ids, max_length=5)\n",
        "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if decoded_output.lower() == 'true':\n",
        "            score = 1.0\n",
        "        elif decoded_output.lower() == 'false':\n",
        "            score = 0.0\n",
        "        else:\n",
        "            score = 0.5  # Set a default score for unexpected output\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def trigger_action(document, retrieved_docs, scores, upper_threshold=0.8, lower_threshold=0.2):\n",
        "    '''\n",
        "    Trigger an appropriate retrieval action based on evaluator scores.\n",
        "\n",
        "    Args:\n",
        "        document (str): The input document\n",
        "        retrieved_docs (list): List of retrieved documents\n",
        "        scores (list): Relevance scores for each retrieved doc\n",
        "        upper_threshold (float): Threshold for triggering Correct action\n",
        "        lower_threshold (float): Threshold for triggering Incorrect action\n",
        "\n",
        "    Returns:\n",
        "        action (str): The triggered action - 'Correct', 'Incorrect' or 'Ambiguous'\n",
        "        selected_docs (list): The documents selected based on the action\n",
        "    '''\n",
        "    if max(scores) > upper_threshold:\n",
        "        # Trigger Correct action\n",
        "        action = 'Correct'\n",
        "        # Select documents with score > upper_threshold\n",
        "        selected_docs = [doc for doc, score in zip(retrieved_docs, scores) if score > upper_threshold]\n",
        "    elif max(scores) < lower_threshold:\n",
        "        # Trigger Incorrect action\n",
        "        action = 'Incorrect'\n",
        "        selected_docs = [] # Discard retrieved docs\n",
        "    else:\n",
        "        # Trigger Ambiguous action\n",
        "        action = 'Ambiguous'\n",
        "        selected_docs = retrieved_docs # Use all retrieved docs\n",
        "\n",
        "    return action, selected_docs\n",
        "\n",
        "# Example - Demonstrating all three triggered actions\n",
        "document = \"The Starry Night is an oil painting by Dutch Post-Impressionist painter Vincent van Gogh.\"\n",
        "retrieved_docs = [\n",
        "    \"The Starry Night is an oil on canvas painting by Dutch Post-Impressionist painter Vincent van Gogh.\",\n",
        "    \"The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci.\",\n",
        "    \"Van Gogh painted The Starry Night in June 1889 while he was a patient in the Saint-Paul-de-Mausole asylum in France.\"\n",
        "]\n",
        "\n",
        "# Get relevance scores from retrieval evaluator\n",
        "scores = evaluate_retrieval(document, retrieved_docs, evaluator_model, tokenizer)\n",
        "\n",
        "# Manually set scores for demonstration purposes\n",
        "scores = [0.9, 0.1, 0.6]\n",
        "\n",
        "# Trigger appropriate action based on scores\n",
        "action, selected_docs = trigger_action(document, retrieved_docs, scores)\n",
        "\n",
        "print(\"Example - Demonstrating all three triggered actions:\")\n",
        "print(f\"Triggered Action: {action}\")\n",
        "print(f\"Selected Documents: {selected_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBOisc1a09Nb",
        "outputId": "ccbad3aa-c9ff-40d2-b393-51b5773b55b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example - Demonstrating all three triggered actions:\n",
            "Triggered Action: Correct\n",
            "Selected Documents: ['The Starry Night is an oil on canvas painting by Dutch Post-Impressionist painter Vincent van Gogh.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer to use as the retrieval evaluator\n",
        "model_name = 't5-large'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "evaluator_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def evaluate_retrieval(document, retrieved_docs, evaluator_model, tokenizer):\n",
        "    '''\n",
        "    Evaluate the relevance of retrieved documents to the input document.\n",
        "\n",
        "    Args:\n",
        "        document (str): The input document\n",
        "        retrieved_docs (list): List of retrieved documents\n",
        "        evaluator_model: The T5 model used as the retrieval evaluator\n",
        "        tokenizer: The T5 tokenizer\n",
        "\n",
        "    Returns:\n",
        "        scores (list): Relevance scores between 0 and 1 for each retrieved doc\n",
        "    '''\n",
        "    scores = []\n",
        "\n",
        "    for doc in retrieved_docs:\n",
        "        # Concatenate document and retrieved doc as input\n",
        "        input_text = f\"Document: {document} Retrieved: {doc} Relevant:\"\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "        # Get relevance score from evaluator model\n",
        "        with torch.no_grad():\n",
        "            outputs = evaluator_model.generate(input_ids, max_length=5)\n",
        "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if decoded_output.lower() == 'true':\n",
        "            score = 1.0\n",
        "        elif decoded_output.lower() == 'false':\n",
        "            score = 0.0\n",
        "        else:\n",
        "            score = 0.5  # Set a default score for unexpected output\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def trigger_action(document, retrieved_docs, scores, upper_threshold=0.8, lower_threshold=0.2):\n",
        "    '''\n",
        "    Trigger an appropriate retrieval action based on evaluator scores.\n",
        "\n",
        "    Args:\n",
        "        document (str): The input document\n",
        "        retrieved_docs (list): List of retrieved documents\n",
        "        scores (list): Relevance scores for each retrieved doc\n",
        "        upper_threshold (float): Threshold for triggering Correct action\n",
        "        lower_threshold (float): Threshold for triggering Incorrect action\n",
        "\n",
        "    Returns:\n",
        "        action (str): The triggered action - 'Correct', 'Incorrect' or 'Ambiguous'\n",
        "        selected_docs (list): The documents selected based on the action\n",
        "    '''\n",
        "    if max(scores) > upper_threshold:\n",
        "        # Trigger Correct action\n",
        "        action = 'Correct'\n",
        "        # Select documents with score > upper_threshold\n",
        "        selected_docs = [doc for doc, score in zip(retrieved_docs, scores) if score > upper_threshold]\n",
        "    elif max(scores) < lower_threshold:\n",
        "        # Trigger Incorrect action\n",
        "        action = 'Incorrect'\n",
        "        selected_docs = [] # Discard retrieved docs\n",
        "    else:\n",
        "        # Trigger Ambiguous action\n",
        "        action = 'Ambiguous'\n",
        "        selected_docs = retrieved_docs # Use all retrieved docs\n",
        "\n",
        "    return action, selected_docs\n",
        "\n",
        "# Example - Demonstrating the \"Incorrect\" triggered action\n",
        "document = \"The Starry Night is an oil painting by Dutch Post-Impressionist painter Vincent van Gogh.\"\n",
        "retrieved_docs = [\n",
        "    \"The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci.\",\n",
        "    \"The Last Supper is a late 15th-century mural painting by Italian artist Leonardo da Vinci.\",\n",
        "    \"The Girl with a Pearl Earring is an oil painting by Dutch Golden Age painter Johannes Vermeer.\"\n",
        "]\n",
        "\n",
        "# Get relevance scores from retrieval evaluator\n",
        "scores = evaluate_retrieval(document, retrieved_docs, evaluator_model, tokenizer)\n",
        "\n",
        "# Manually set scores for demonstration purposes\n",
        "scores = [0.1, 0.05, 0.15]\n",
        "\n",
        "# Trigger appropriate action based on scores\n",
        "action, selected_docs = trigger_action(document, retrieved_docs, scores)\n",
        "\n",
        "print(\"Example - Demonstrating the 'Incorrect' triggered action:\")\n",
        "print(f\"Triggered Action: {action}\")\n",
        "print(f\"Selected Documents: {selected_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbbZL4wv1aey",
        "outputId": "2db09420-5ced-460f-f395-99688d2f29dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example - Demonstrating the 'Incorrect' triggered action:\n",
            "Triggered Action: Incorrect\n",
            "Selected Documents: []\n"
          ]
        }
      ]
    }
  ]
}