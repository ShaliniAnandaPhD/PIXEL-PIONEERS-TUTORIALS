{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNidUcGp+jufafFdi1ynnBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/PIXEL-PIONEERS-TUTORIALS/blob/main/Assistive_Robot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AssistiveRobot Simulation\n",
        "This notebook simulates an Assistive Robot that learns from its actions using a simple feedback loop. The robot performs tasks, receives feedback, and adjusts its actions based on environmental factors and its energy levels. The robot's iterative learning process, as described, aligns with the \"LearAct\" paper's core principles. It illustrates how a robot can dynamically adapt its actions and strategies based on feedback from the environment, embodying the paper's focus on enhancing task performance through experiential learning and adaptive action refinement. This simulation showcases the practical application of the \"LearAct\" framework's theoretical concepts, demonstrating the potential for robots to improve autonomously over time by adjusting their behavior in response to environmental challenges and outcomes.\n"
      ],
      "metadata": {
        "id": "I6hF7l00qB0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n"
      ],
      "metadata": {
        "id": "bsIq_6rwwRo4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `AssistiveRobot` class and its functions are designed as follows:\n",
        "\n",
        "- `__init__`: Initializes the robot with a task knowledge base, action memory, environmental factors, and energy levels.\n",
        "- `perform_task`: Orchestrates task execution, managing energy, and adapting to environmental contexts.\n",
        "- `execute_action`: Simulates the performance of an action, considering the current environmental situation.\n",
        "- `simulate_feedback`: Generates simulated feedback for actions based on success or failure probabilities.\n",
        "- `learn_from_feedback`: Updates the robot's approach based on the feedback received from action outcomes.\n",
        "- `adjust_action`: Modifies the robot's strategy when an action fails, aiming to improve future performance.\n",
        "- `simulate_environmental_context`: Randomly alters environmental factors to affect the success of actions.\n",
        "- `change_strategy`: Implements changes in the robot's approach when standard adjustments fail, considering environmental challenges.\n",
        "\n",
        "This setup showcases a basic learning mechanism where the robot adapts its behavior over time, aiming to improve task efficiency and effectiveness within a simulated environment."
      ],
      "metadata": {
        "id": "GhBinybD1H7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class AssistiveRobot:\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = {'fetch': ['approach', 'grasp', 'retrieve']}\n",
        "        self.action_memory = {}\n",
        "        self.environmental_factors = {'obstacle': False, 'distance': 'close'}\n",
        "        self.energy_levels = 100  # Initialize energy levels\n",
        "        self.current_state = 'idle'  # Possible states: idle, navigating, manipulating\n",
        "\n",
        "    def perform_task(self, task, priority=1):\n",
        "        if self.energy_levels < 20:\n",
        "            print(\"Energy too low to perform task.\")\n",
        "            return\n",
        "        self.current_state = 'navigating' if task == 'fetch' else 'manipulating'\n",
        "        actions = self.knowledge_base.get(task, [])\n",
        "        for action in actions:\n",
        "            self.execute_action(action)\n",
        "            self.learn_from_feedback(action)\n",
        "            # Energy consumption simulation\n",
        "            self.energy_levels -= random.randint(1, 5)\n",
        "            print(f\"Energy levels now at: {self.energy_levels}\")\n",
        "            if self.energy_levels < 20:\n",
        "                print(\"Energy too low, returning to base for recharge.\")\n",
        "                break\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        env_factor = self.simulate_environmental_context(action)\n",
        "        print(f\"Executing {action}, Environmental Context: {env_factor}, Current State: {self.current_state}\")\n",
        "\n",
        "    def simulate_feedback(self, action):\n",
        "        if self.action_memory.get(action, 0) < -2 or self.environmental_factors['obstacle']:\n",
        "            return False\n",
        "        else:\n",
        "            return random.choice([True] * 3 + [False])\n",
        "\n",
        "    def learn_from_feedback(self, action):\n",
        "        success = self.simulate_feedback(action)\n",
        "        self.action_memory[action] = self.action_memory.get(action, 0) + (1 if success else -1)\n",
        "        print(f\"Action '{action}' was {'successful' if success else 'unsuccessful'}, Memory: {self.action_memory[action]}\")\n",
        "        if not success:\n",
        "            self.adjust_action(action)\n",
        "        else:\n",
        "            # Reinforce successful action by increasing memory count\n",
        "            self.action_memory[action] += 1\n",
        "\n",
        "    def adjust_action(self, action):\n",
        "        if 'retry-' + action not in self.knowledge_base['fetch']:\n",
        "            self.knowledge_base['fetch'].insert(0, 'retry-' + action)\n",
        "        else:\n",
        "            self.change_strategy(action)\n",
        "\n",
        "    def simulate_environmental_context(self, action):\n",
        "        self.environmental_factors['obstacle'] = random.choice([True, False])\n",
        "        self.environmental_factors['distance'] = random.choice(['close', 'far'])\n",
        "        return self.environmental_factors\n",
        "\n",
        "    def change_strategy(self, action):\n",
        "        print(f\"Changing strategy for {action}, considering environmental factors.\")\n",
        "        if self.environmental_factors['distance'] == 'far':\n",
        "            self.knowledge_base['fetch'].insert(0, 'move-closer')\n",
        "        elif self.environmental_factors['obstacle']:\n",
        "            self.knowledge_base['fetch'].insert(0, 'remove-obstacle')\n",
        "\n",
        "# Create and test an instance of AssistiveRobot\n",
        "robot = AssistiveRobot()\n",
        "robot.perform_task('fetch')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX3OTN0HybO9",
        "outputId": "b4db86a9-a1f3-4c01-b094-57f63605874b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing approach, Environmental Context: {'obstacle': True, 'distance': 'far'}, Current State: navigating\n",
            "Action 'approach' was unsuccessful, Memory: -1\n",
            "Energy levels now at: 95\n",
            "Executing approach, Environmental Context: {'obstacle': False, 'distance': 'far'}, Current State: navigating\n",
            "Action 'approach' was successful, Memory: 0\n",
            "Energy levels now at: 92\n",
            "Executing grasp, Environmental Context: {'obstacle': True, 'distance': 'far'}, Current State: navigating\n",
            "Action 'grasp' was unsuccessful, Memory: -1\n",
            "Energy levels now at: 90\n",
            "Executing grasp, Environmental Context: {'obstacle': True, 'distance': 'far'}, Current State: navigating\n",
            "Action 'grasp' was unsuccessful, Memory: -2\n",
            "Changing strategy for grasp, considering environmental factors.\n",
            "Energy levels now at: 86\n",
            "Executing grasp, Environmental Context: {'obstacle': True, 'distance': 'far'}, Current State: navigating\n",
            "Action 'grasp' was unsuccessful, Memory: -3\n",
            "Changing strategy for grasp, considering environmental factors.\n",
            "Energy levels now at: 82\n",
            "Executing grasp, Environmental Context: {'obstacle': True, 'distance': 'close'}, Current State: navigating\n",
            "Action 'grasp' was unsuccessful, Memory: -4\n",
            "Changing strategy for grasp, considering environmental factors.\n",
            "Energy levels now at: 80\n",
            "Executing grasp, Environmental Context: {'obstacle': True, 'distance': 'far'}, Current State: navigating\n",
            "Action 'grasp' was unsuccessful, Memory: -5\n",
            "Changing strategy for grasp, considering environmental factors.\n",
            "Energy levels now at: 77\n",
            "Executing grasp, Environmental Context: {'obstacle': False, 'distance': 'far'}, Current State: navigating\n",
            "Action 'grasp' was unsuccessful, Memory: -6\n",
            "Changing strategy for grasp, considering environmental factors.\n",
            "Energy levels now at: 72\n",
            "Executing grasp, Environmental Context: {'obstacle': False, 'distance': 'close'}, Current State: navigating\n",
            "Action 'grasp' was unsuccessful, Memory: -7\n",
            "Changing strategy for grasp, considering environmental factors.\n",
            "Energy levels now at: 67\n",
            "Executing retrieve, Environmental Context: {'obstacle': True, 'distance': 'far'}, Current State: navigating\n",
            "Action 'retrieve' was unsuccessful, Memory: -1\n",
            "Energy levels now at: 62\n",
            "Executing retrieve, Environmental Context: {'obstacle': False, 'distance': 'close'}, Current State: navigating\n",
            "Action 'retrieve' was successful, Memory: 0\n",
            "Energy levels now at: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUICK RECAP OF WHAT THE RESULTS MEAN:"
      ],
      "metadata": {
        "id": "Ehma-KT120bN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Initial Energy: Robot starts with energy at 95.\n",
        "- First Task: Successfully approaches target, energy drops to 92.\n",
        "- Grasping Attempts: Faces obstacles, attempts to grasp fail, leading to strategy changes; energy reduces sequentially with each attempt (90, 86, 82, 80, 77, 72, 67).\n",
        "- Learning Through Failure: Each failed grasp attempt adjusts the robot's memory negatively, indicating learning from unsuccessful actions.\n",
        "- Final Task: Successful retrieval in a close, obstacle-free environment, proving adaptability; ends with energy at 59.\n",
        "- Adaptive Strategy: Changes in strategy after repeated failures demonstrate the robot's ability to adapt based on environmental feedback and memory adjustments.\n",
        "- Energy Management: Continuous energy depletion showcases the need for efficient action execution and strategy refinement."
      ],
      "metadata": {
        "id": "b9D_qvnw2klj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Robot Task Execution and Learning Summary with Numerical Interpretations**\n",
        "\n",
        "- **Action Execution:** Initiates tasks within a simulated environment, confronting obstacles and variable distances to targets.\n",
        "  \n",
        "- **Environmental Challenges:** Evaluates each action within specific contexts, like obstacle presence or target distance, impacting success rates.\n",
        "\n",
        "- **Feedback and Adaptation:** Receives and adapts to action feedback, with strategy modifications informed by success or failure, noted in memory adjustments.\n",
        "\n",
        "- **Energy Management:** Monitors decreasing energy with each action, necessitating strategic adjustments for task continuation.\n",
        "\n",
        "- **Numerical Interpretations:**\n",
        "  - **Memory Values:** Negative numbers indicate learning from unsuccessful actions, guiding strategy refinement.\n",
        "  - **Energy Levels:** Displayed post-action, these numbers reflect remaining operational capacity, influencing task continuation decisions.\n",
        "\n",
        "- **Learning Process:** Demonstrates adaptability through strategy evolution post-failure, highlighting an iterative learning mechanism.\n",
        "\n",
        "- **Successful Outcomes:** Achieves task completion under optimal conditions, overcoming environmental constraints and showcasing adaptability.\n",
        "\n",
        "This overview captures the robot's adaptive learning cycle, including task execution, environmental navigation, feedback processing, and energy management, underscored by numerical feedback for continuous improvement."
      ],
      "metadata": {
        "id": "CBTLWOpW3D1-"
      }
    }
  ]
}