{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhswHpMo57IeG4f2HdYJz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/PIXEL-PIONEERS-TUTORIALS/blob/main/Advanages_of_Monosemanticity_in_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notion page with more explaination: [Notion page](https://www.notion.so/shalini-ananda-phd/Unraveling-the-Mysteries-of-Machine-Learning-A-Monosemantic-Approach-fef2afbbd50d4910a62b174a87927ec2)"
      ],
      "metadata": {
        "id": "SpMjPuSjuA17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2LHfogCUNBK",
        "outputId": "f1ec00f3-67e7-4e93-e3f8-7c08c4b1ab4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import DictionaryLearning\n"
      ],
      "metadata": {
        "id": "P_qtXNQgV8gV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(exemption_notices_path, working_forest_path):\n",
        "    # Load the datasets\n",
        "    exemption_notices = pd.read_csv(exemption_notices_path)\n",
        "    working_forest = pd.read_csv(working_forest_path)\n",
        "\n",
        "    # Display the column names to understand the structure\n",
        "    print(\"Exemption Notices Columns:\")\n",
        "    print(exemption_notices.columns)\n",
        "    print(\"\\nWorking Forest Columns:\")\n",
        "    print(working_forest.columns)\n",
        "\n",
        "    # Assuming 'OBJECTID' is the common key for merging\n",
        "    common_key = 'OBJECTID'  # Replace this with the actual common key if different\n",
        "\n",
        "    # Check if the common key exists in both dataframes\n",
        "    if common_key not in exemption_notices.columns or common_key not in working_forest.columns:\n",
        "        raise KeyError(f\"'{common_key}' not found in one of the dataframes\")\n",
        "\n",
        "    # Merge datasets on the common key\n",
        "    data = pd.merge(exemption_notices, working_forest, on=common_key, suffixes=('_exempt', '_forest'))\n",
        "\n",
        "    # Display the merged data columns to check for GIS_ACRES\n",
        "    print(\"Merged Data Columns:\")\n",
        "    print(data.columns)\n",
        "\n",
        "    # Ensure GIS_ACRES is included in the merged dataset\n",
        "    if 'GIS_ACRES_exempt' not in data.columns and 'GIS_ACRES_forest' not in data.columns:\n",
        "        raise KeyError(\"'GIS_ACRES' column not found in the merged dataframe\")\n",
        "\n",
        "    # Choose the appropriate GIS_ACRES column\n",
        "    if 'GIS_ACRES_exempt' in data.columns:\n",
        "        labels = data['GIS_ACRES_exempt']\n",
        "        features = data.drop(columns=['GIS_ACRES_exempt'])\n",
        "    else:\n",
        "        labels = data['GIS_ACRES_forest']\n",
        "        features = data.drop(columns=['GIS_ACRES_forest'])\n",
        "\n",
        "    # Perform data cleaning and preprocessing\n",
        "    # Example preprocessing steps:\n",
        "    features = features.fillna(0)  # Fill missing values with 0\n",
        "    features = features.select_dtypes(include=[int, float])  # Select only numerical columns\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Paths to the provided datasets\n",
        "exemption_notices_path = \"/content/CAL_FIRE_Exemption_Notices_All_TA83_580667334888250359.csv\"\n",
        "working_forest_path = \"/content/CAL_FIRE_Working_Forest_Management_Plans_and_Notices_TA83_8835581698934070752.csv\"\n",
        "\n",
        "# Load and preprocess the data\n",
        "features, labels = load_data(exemption_notices_path, working_forest_path)\n",
        "\n",
        "# Display the first few rows of features and labels to verify\n",
        "print(\"Features:\")\n",
        "print(features.head())\n",
        "print(\"\\nLabels:\")\n",
        "print(labels.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTCOPuq0agX0",
        "outputId": "7e5e1261-0915-4841-f272-adf57b52b028"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemption Notices Columns:\n",
            "Index(['OBJECTID', 'REPORTD_AC', 'REGION', 'EX_YEAR', 'EX_NUM', 'COUNTY',\n",
            "       'LANDOWNER', 'EX_TYPE', 'ACCEPTED', 'EXPIRATION', 'COMPLETED',\n",
            "       'COMMENTS', 'GIS_ACRES', 'HD_NUM', 'GLOBALID', 'Shape__Area',\n",
            "       'Shape__Length'],\n",
            "      dtype='object')\n",
            "\n",
            "Working Forest Columns:\n",
            "Index(['OBJECTID', 'GIS_ACRES', 'REGION', 'WFMP_YEAR', 'WFN_YEAR', 'WFMP_NUM',\n",
            "       'WFN_NUM', 'COUNTY', 'TIMBEROWNR', 'LANDOWNER', 'SILVI_1', 'SILVI_2',\n",
            "       'SILVI_CAT', 'YARD', 'UNIT', 'PLAN_STAT', 'ACCEPTED', 'COMPLETED',\n",
            "       'COMMENTS', 'SPATL_MOD', 'MODIFIED', 'HD_NUM', 'GLOBALID',\n",
            "       'Shape__Area', 'Shape__Length'],\n",
            "      dtype='object')\n",
            "Merged Data Columns:\n",
            "Index(['OBJECTID', 'REPORTD_AC', 'REGION_exempt', 'EX_YEAR', 'EX_NUM',\n",
            "       'COUNTY_exempt', 'LANDOWNER_exempt', 'EX_TYPE', 'ACCEPTED_exempt',\n",
            "       'EXPIRATION', 'COMPLETED_exempt', 'COMMENTS_exempt', 'GIS_ACRES_exempt',\n",
            "       'HD_NUM_exempt', 'GLOBALID_exempt', 'Shape__Area_exempt',\n",
            "       'Shape__Length_exempt', 'GIS_ACRES_forest', 'REGION_forest',\n",
            "       'WFMP_YEAR', 'WFN_YEAR', 'WFMP_NUM', 'WFN_NUM', 'COUNTY_forest',\n",
            "       'TIMBEROWNR', 'LANDOWNER_forest', 'SILVI_1', 'SILVI_2', 'SILVI_CAT',\n",
            "       'YARD', 'UNIT', 'PLAN_STAT', 'ACCEPTED_forest', 'COMPLETED_forest',\n",
            "       'COMMENTS_forest', 'SPATL_MOD', 'MODIFIED', 'HD_NUM_forest',\n",
            "       'GLOBALID_forest', 'Shape__Area_forest', 'Shape__Length_forest'],\n",
            "      dtype='object')\n",
            "Features:\n",
            "   OBJECTID  REPORTD_AC  REGION_exempt  EX_YEAR  EX_NUM  COMPLETED_exempt  \\\n",
            "0         6        6.10              4     2017       8               0.0   \n",
            "1         7        0.58              4     2017       9               0.0   \n",
            "2         8       20.24              2     2017     107               0.0   \n",
            "3         9        5.00              4     2017      35               0.0   \n",
            "4        10        4.29              2     2017     158               0.0   \n",
            "\n",
            "   Shape__Area_exempt  Shape__Length_exempt  GIS_ACRES_forest  REGION_forest  \\\n",
            "0        32754.729438           1604.150800          5.246688              2   \n",
            "1         2341.684523            334.090914          4.800579              2   \n",
            "2       130438.615568           4154.407241       1896.003184              2   \n",
            "3        12598.764975            876.687803        472.065312              2   \n",
            "4        18353.869456           1165.249291         20.807683              2   \n",
            "\n",
            "   WFMP_YEAR  WFN_YEAR  WFMP_NUM  WFN_NUM  SILVI_2  UNIT  SPATL_MOD  MODIFIED  \\\n",
            "0       2019      2021         1        1      0.0   0.0        0.0       0.0   \n",
            "1       2019      2021         1        1      0.0   0.0        0.0       0.0   \n",
            "2       2019      2023         1        2      0.0   0.0        0.0       0.0   \n",
            "3       2019      2023         1        3      0.0   0.0        0.0       0.0   \n",
            "4       2019      2023         1        3      0.0   0.0        0.0       0.0   \n",
            "\n",
            "   Shape__Area_forest  Shape__Length_forest  \n",
            "0        2.123268e+04            624.758990  \n",
            "1        1.942733e+04            578.918971  \n",
            "2        7.672883e+06          25561.442892  \n",
            "3        1.910388e+06          12737.268304  \n",
            "4        8.420604e+04           1325.282713  \n",
            "\n",
            "Labels:\n",
            "0     8.093872\n",
            "1     0.578643\n",
            "2    32.232087\n",
            "3     3.113221\n",
            "4     4.535339\n",
            "Name: GIS_ACRES_exempt, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import DictionaryLearning\n",
        "import pandas as pd\n",
        "\n",
        "# Function to map neuron activation patterns\n",
        "def map_activation_patterns(features):\n",
        "    # Ensure there are no empty rows\n",
        "    features = features[features.any(axis=1)]\n",
        "\n",
        "    # Apply dictionary learning to extract features\n",
        "    dict_learner = DictionaryLearning(n_components=10, random_state=42)  # Adjust n_components as needed\n",
        "    transformed_features = dict_learner.fit_transform(features)\n",
        "    return transformed_features\n",
        "\n",
        "# Map neuron activation patterns\n",
        "mapped_features = map_activation_patterns(features)\n",
        "\n",
        "# Display the first few rows of the mapped features to verify\n",
        "print(\"Mapped Features:\")\n",
        "print(pd.DataFrame(mapped_features).head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8RR7fHpa3eK",
        "outputId": "ac477f61-ea6f-40c7-cebe-455ce40bbe92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped Features:\n",
            "              0              1    2    3    4           5             6  \\\n",
            "0  0.000000e+00       0.000000  0.0  0.0  0.0     0.00000 -10060.416654   \n",
            "1  0.000000e+00       0.000000  0.0  0.0  0.0 -3637.22492      0.000000   \n",
            "2  7.673137e+06 -121704.095856  0.0  0.0  0.0     0.00000      0.000000   \n",
            "3  1.909834e+06  -50133.525746  0.0  0.0  0.0     0.00000      0.000000   \n",
            "4  0.000000e+00    4187.928254  0.0  0.0  0.0     0.00000      0.000000   \n",
            "\n",
            "              7    8             9  \n",
            "0      0.000000  0.0  44720.518974  \n",
            "1  17449.408054  0.0      0.000000  \n",
            "2      0.000000  0.0      0.000000  \n",
            "3      0.000000  0.0      0.000000  \n",
            "4  85535.818651  0.0      0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def train_predictive_model(features, labels):\n",
        "    # Convert to numpy arrays\n",
        "    X_train = features\n",
        "    y_train = labels.values\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_train)\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = np.mean((y_train - y_pred) ** 2)\n",
        "    mae = np.mean(np.abs(y_train - y_pred))\n",
        "\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Convert mapped features to DataFrame\n",
        "mapped_features_df = pd.DataFrame(mapped_features)\n",
        "\n",
        "# Train the predictive model\n",
        "model = train_predictive_model(mapped_features_df, labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNAQHD2xbZ3E",
        "outputId": "634f3393-9f40-4810-94dc-91328453517b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 17210857472.0000 - mae: 70129.2422 - val_loss: 3036160768.0000 - val_mae: 41325.1953\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 28809928704.0000 - mae: 77188.2109 - val_loss: 2416389632.0000 - val_mae: 36606.1758\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 5728389632.0000 - mae: 46733.1992 - val_loss: 1871158016.0000 - val_mae: 31905.7539\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1343555200.0000 - mae: 23637.8848 - val_loss: 1405134208.0000 - val_mae: 27366.6211\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 30522331136.0000 - mae: 76559.1172 - val_loss: 1049571968.0000 - val_mae: 23735.5664\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 26281287680.0000 - mae: 64636.1172 - val_loss: 930900416.0000 - val_mae: 22825.1387\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 22968481792.0000 - mae: 71354.6641 - val_loss: 748709312.0000 - val_mae: 21198.2168\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 50650038272.0000 - mae: 83753.7031 - val_loss: 800063616.0000 - val_mae: 21291.1250\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 7029943296.0000 - mae: 45774.8516 - val_loss: 869872512.0000 - val_mae: 20338.9141\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 935346368.0000 - mae: 19231.7461 - val_loss: 1033802496.0000 - val_mae: 21489.6973\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "Mean Squared Error: 1150168445.9265\n",
            "Mean Absolute Error: 18946.3766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_custom_response(model, data, threshold=50):\n",
        "    # Use the trained model to generate predictions\n",
        "    predictions = model.predict(data)\n",
        "    # Generate recommendations based on the threshold\n",
        "    recommendations = [\"High Risk\" if pred > threshold else \"Low Risk\" for pred in predictions.flatten()]\n",
        "    return recommendations\n",
        "\n",
        "# Generate custom responses using the trained model and mapped features\n",
        "recommendations = generate_custom_response(model, pd.DataFrame(mapped_features))\n",
        "\n",
        "# Display the generated recommendations\n",
        "print(\"Recommendations:\")\n",
        "print(recommendations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvuzfBVrbeHL",
        "outputId": "afbaea59-1b1e-404a-8d19-61e47a740484"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "Recommendations:\n",
            "['High Risk', 'Low Risk', 'Low Risk', 'Low Risk', 'Low Risk', 'High Risk', 'High Risk', 'High Risk', 'Low Risk', 'High Risk', 'High Risk', 'High Risk', 'Low Risk', 'High Risk', 'High Risk', 'Low Risk', 'High Risk', 'High Risk', 'High Risk', 'High Risk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_support_system(model, data, threshold=50):\n",
        "    # Use the trained model to generate predictions\n",
        "    predictions = model.predict(data)\n",
        "\n",
        "    decisions, alerts = [], []\n",
        "\n",
        "    for pred in predictions.flatten():\n",
        "        if pred > threshold:\n",
        "            decisions.append(\"Deploy Resources\")\n",
        "            alerts.append(\"High Risk: Evacuate Area\")\n",
        "        else:\n",
        "            decisions.append(\"Monitor Area\")\n",
        "            alerts.append(\"Low Risk: No Immediate Action Required\")\n",
        "\n",
        "    return decisions, alerts\n",
        "\n",
        "# Implement decision support system\n",
        "decisions, alerts = decision_support_system(model, pd.DataFrame(mapped_features))\n",
        "\n",
        "# Display decisions and alerts\n",
        "print(\"Decisions:\")\n",
        "print(decisions)\n",
        "print(\"\\nAlerts:\")\n",
        "print(alerts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HMD89ZKbyqS",
        "outputId": "e2d0eca2-6bea-490e-d746-22834796867a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "Decisions:\n",
            "['Deploy Resources', 'Monitor Area', 'Monitor Area', 'Monitor Area', 'Monitor Area', 'Deploy Resources', 'Deploy Resources', 'Deploy Resources', 'Monitor Area', 'Deploy Resources', 'Deploy Resources', 'Deploy Resources', 'Monitor Area', 'Deploy Resources', 'Deploy Resources', 'Monitor Area', 'Deploy Resources', 'Deploy Resources', 'Deploy Resources', 'Deploy Resources']\n",
            "\n",
            "Alerts:\n",
            "['High Risk: Evacuate Area', 'Low Risk: No Immediate Action Required', 'Low Risk: No Immediate Action Required', 'Low Risk: No Immediate Action Required', 'Low Risk: No Immediate Action Required', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area', 'Low Risk: No Immediate Action Required', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area', 'Low Risk: No Immediate Action Required', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area', 'Low Risk: No Immediate Action Required', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area', 'High Risk: Evacuate Area']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NON MONOSEMANTIC"
      ],
      "metadata": {
        "id": "CSRir6s4l9XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "def load_and_preprocess_data(exemption_notices_path, working_forest_path):\n",
        "    # Load datasets\n",
        "    exemption_notices = pd.read_csv(exemption_notices_path)\n",
        "    working_forest_data = pd.read_csv(working_forest_path)\n",
        "\n",
        "    # Merge datasets on 'OBJECTID'\n",
        "    merged_data = pd.merge(exemption_notices, working_forest_data, on='OBJECTID')\n",
        "\n",
        "    # Inspect the columns to choose relevant features\n",
        "    print(\"Merged Data Columns:\", merged_data.columns)\n",
        "\n",
        "    # Select relevant columns and handle missing values\n",
        "    features = merged_data[['REPORTD_AC', 'EX_YEAR', 'GIS_ACRES_x']].fillna(0)\n",
        "    labels = merged_data['PLAN_STAT']\n",
        "\n",
        "    # Encode string labels to numerical values\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = MinMaxScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Check for data imbalance\n",
        "    print(\"Label distribution:\", pd.Series(labels_encoded).value_counts())\n",
        "\n",
        "    return features_scaled, labels_encoded\n",
        "\n",
        "# Paths to the provided datasets\n",
        "exemption_notices_path = \"/content/CAL_FIRE_Exemption_Notices_All_TA83_580667334888250359.csv\"\n",
        "working_forest_path = \"/content/CAL_FIRE_Working_Forest_Management_Plans_and_Notices_TA83_8835581698934070752.csv\"\n",
        "\n",
        "# Load and preprocess data\n",
        "features, labels = load_and_preprocess_data(exemption_notices_path, working_forest_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLtmlEGmnZCL",
        "outputId": "a459bd89-9191-4b33-a771-fa26e342423c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged Data Columns: Index(['OBJECTID', 'REPORTD_AC', 'REGION_x', 'EX_YEAR', 'EX_NUM', 'COUNTY_x',\n",
            "       'LANDOWNER_x', 'EX_TYPE', 'ACCEPTED_x', 'EXPIRATION', 'COMPLETED_x',\n",
            "       'COMMENTS_x', 'GIS_ACRES_x', 'HD_NUM_x', 'GLOBALID_x', 'Shape__Area_x',\n",
            "       'Shape__Length_x', 'GIS_ACRES_y', 'REGION_y', 'WFMP_YEAR', 'WFN_YEAR',\n",
            "       'WFMP_NUM', 'WFN_NUM', 'COUNTY_y', 'TIMBEROWNR', 'LANDOWNER_y',\n",
            "       'SILVI_1', 'SILVI_2', 'SILVI_CAT', 'YARD', 'UNIT', 'PLAN_STAT',\n",
            "       'ACCEPTED_y', 'COMPLETED_y', 'COMMENTS_y', 'SPATL_MOD', 'MODIFIED',\n",
            "       'HD_NUM_y', 'GLOBALID_y', 'Shape__Area_y', 'Shape__Length_y'],\n",
            "      dtype='object')\n",
            "Label distribution: 2    9\n",
            "0    8\n",
            "1    3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_data(exemption_notices_path, working_forest_path):\n",
        "    # Load datasets\n",
        "    exemption_notices = pd.read_csv(exemption_notices_path)\n",
        "    working_forest_data = pd.read_csv(working_forest_path)\n",
        "\n",
        "    # Merge datasets on 'OBJECTID'\n",
        "    merged_data = pd.merge(exemption_notices, working_forest_data, on='OBJECTID')\n",
        "\n",
        "    # Inspect the columns to choose relevant features\n",
        "    print(\"Merged Data Columns:\", merged_data.columns)\n",
        "\n",
        "    # Select relevant columns and handle missing values\n",
        "    features = merged_data[['REPORTD_AC', 'EX_YEAR', 'GIS_ACRES_x', 'WFMP_YEAR', 'WFN_YEAR', 'YARD', 'SILVI_1', 'SILVI_2']].fillna(0)\n",
        "    labels = merged_data['PLAN_STAT']\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    features = pd.get_dummies(features, columns=['YARD', 'SILVI_1', 'SILVI_2'], drop_first=True)\n",
        "\n",
        "    # Encode string labels to numerical values\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = MinMaxScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Check for data imbalance\n",
        "    print(\"Label distribution before SMOTE:\", pd.Series(labels_encoded).value_counts())\n",
        "\n",
        "    # Apply SMOTE to balance the classes with adjusted n_neighbors\n",
        "    smote = SMOTE(random_state=42, k_neighbors=2)\n",
        "    features_resampled, labels_resampled = smote.fit_resample(features_scaled, labels_encoded)\n",
        "\n",
        "    print(\"Label distribution after SMOTE:\", pd.Series(labels_resampled).value_counts())\n",
        "\n",
        "    return features_resampled, labels_resampled\n",
        "\n",
        "# Paths to the provided datasets\n",
        "exemption_notices_path = \"/content/CAL_FIRE_Exemption_Notices_All_TA83_580667334888250359.csv\"\n",
        "working_forest_path = \"/content/CAL_FIRE_Working_Forest_Management_Plans_and_Notices_TA83_8835581698934070752.csv\"\n",
        "\n",
        "# Load and preprocess data\n",
        "features, labels = load_and_preprocess_data(exemption_notices_path, working_forest_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgepo5xxoFTY",
        "outputId": "9cb30768-994e-47aa-83a9-47b65e3a7b25"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged Data Columns: Index(['OBJECTID', 'REPORTD_AC', 'REGION_x', 'EX_YEAR', 'EX_NUM', 'COUNTY_x',\n",
            "       'LANDOWNER_x', 'EX_TYPE', 'ACCEPTED_x', 'EXPIRATION', 'COMPLETED_x',\n",
            "       'COMMENTS_x', 'GIS_ACRES_x', 'HD_NUM_x', 'GLOBALID_x', 'Shape__Area_x',\n",
            "       'Shape__Length_x', 'GIS_ACRES_y', 'REGION_y', 'WFMP_YEAR', 'WFN_YEAR',\n",
            "       'WFMP_NUM', 'WFN_NUM', 'COUNTY_y', 'TIMBEROWNR', 'LANDOWNER_y',\n",
            "       'SILVI_1', 'SILVI_2', 'SILVI_CAT', 'YARD', 'UNIT', 'PLAN_STAT',\n",
            "       'ACCEPTED_y', 'COMPLETED_y', 'COMMENTS_y', 'SPATL_MOD', 'MODIFIED',\n",
            "       'HD_NUM_y', 'GLOBALID_y', 'Shape__Area_y', 'Shape__Length_y'],\n",
            "      dtype='object')\n",
            "Label distribution before SMOTE: 2    9\n",
            "0    8\n",
            "1    3\n",
            "Name: count, dtype: int64\n",
            "Label distribution after SMOTE: 2    9\n",
            "0    9\n",
            "1    9\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_data(exemption_notices_path, working_forest_path):\n",
        "    # Load datasets\n",
        "    exemption_notices = pd.read_csv(exemption_notices_path)\n",
        "    working_forest_data = pd.read_csv(working_forest_path)\n",
        "\n",
        "    # Merge datasets on 'OBJECTID'\n",
        "    merged_data = pd.merge(exemption_notices, working_forest_data, on='OBJECTID')\n",
        "\n",
        "    # Inspect the columns to choose relevant features\n",
        "    print(\"Merged Data Columns:\", merged_data.columns)\n",
        "\n",
        "    # Select relevant columns and handle missing values\n",
        "    features = merged_data[['REPORTD_AC', 'EX_YEAR', 'GIS_ACRES_x', 'WFMP_YEAR', 'WFN_YEAR', 'YARD', 'SILVI_1', 'SILVI_2']].fillna(0)\n",
        "    labels = merged_data['PLAN_STAT']\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    features = pd.get_dummies(features, columns=['YARD', 'SILVI_1', 'SILVI_2'], drop_first=True)\n",
        "\n",
        "    # Encode string labels to numerical values\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = MinMaxScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Check for data imbalance\n",
        "    print(\"Label distribution before SMOTE:\", pd.Series(labels_encoded).value_counts())\n",
        "\n",
        "    # Apply SMOTE to balance the classes with adjusted n_neighbors\n",
        "    smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "    features_resampled, labels_resampled = smote.fit_resample(features_scaled, labels_encoded)\n",
        "\n",
        "    print(\"Label distribution after SMOTE:\", pd.Series(labels_resampled).value_counts())\n",
        "\n",
        "    return features_resampled, labels_resampled\n",
        "\n",
        "# Paths to the provided datasets\n",
        "exemption_notices_path = \"/content/CAL_FIRE_Exemption_Notices_All_TA83_580667334888250359.csv\"\n",
        "working_forest_path = \"/content/CAL_FIRE_Working_Forest_Management_Plans_and_Notices_TA83_8835581698934070752.csv\"\n",
        "\n",
        "# Load and preprocess data\n",
        "features, labels = load_and_preprocess_data(exemption_notices_path, working_forest_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1hk_8zlocGY",
        "outputId": "8ed6f104-21e8-4afb-a1e5-31a444442e50"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged Data Columns: Index(['OBJECTID', 'REPORTD_AC', 'REGION_x', 'EX_YEAR', 'EX_NUM', 'COUNTY_x',\n",
            "       'LANDOWNER_x', 'EX_TYPE', 'ACCEPTED_x', 'EXPIRATION', 'COMPLETED_x',\n",
            "       'COMMENTS_x', 'GIS_ACRES_x', 'HD_NUM_x', 'GLOBALID_x', 'Shape__Area_x',\n",
            "       'Shape__Length_x', 'GIS_ACRES_y', 'REGION_y', 'WFMP_YEAR', 'WFN_YEAR',\n",
            "       'WFMP_NUM', 'WFN_NUM', 'COUNTY_y', 'TIMBEROWNR', 'LANDOWNER_y',\n",
            "       'SILVI_1', 'SILVI_2', 'SILVI_CAT', 'YARD', 'UNIT', 'PLAN_STAT',\n",
            "       'ACCEPTED_y', 'COMPLETED_y', 'COMMENTS_y', 'SPATL_MOD', 'MODIFIED',\n",
            "       'HD_NUM_y', 'GLOBALID_y', 'Shape__Area_y', 'Shape__Length_y'],\n",
            "      dtype='object')\n",
            "Label distribution before SMOTE: 2    9\n",
            "0    8\n",
            "1    3\n",
            "Name: count, dtype: int64\n",
            "Label distribution after SMOTE: 2    9\n",
            "0    9\n",
            "1    9\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# Build simplified neural network model\n",
        "non_monosemantic_model = Sequential([\n",
        "    Dense(64, input_dim=features.shape[1], activation='relu'),  # Adjust input_dim to match the number of features\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Use sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile model with a lower learning rate\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "non_monosemantic_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history_non_monosemantic = non_monosemantic_model.fit(features, labels, epochs=50, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = non_monosemantic_model.evaluate(features, labels)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate custom responses\n",
        "def generate_custom_response_non_monosemantic(model, data, threshold=0.5):\n",
        "    predictions = model.predict(data)\n",
        "    responses = [\"High Risk\" if pred > threshold else \"Low Risk\" for pred in predictions]\n",
        "    return responses\n",
        "\n",
        "# Generate responses\n",
        "responses_non_monosemantic = generate_custom_response_non_monosemantic(non_monosemantic_model, features)\n",
        "\n",
        "# Display sample predictions and responses\n",
        "for i, (pred, resp) in enumerate(zip(non_monosemantic_model.predict(features), responses_non_monosemantic)):\n",
        "    print(f\"Data Point {i+1}: Predicted Risk = {pred[0]:.2f}, Response = {resp}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "putobykroeOf",
        "outputId": "60d80a51-1e6d-45ce-eec6-991ef98d3625"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2/2 [==============================] - 1s 240ms/step - loss: 0.7313 - accuracy: 0.3333 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.6947 - accuracy: 0.4286 - val_loss: 0.6879 - val_accuracy: 1.0000\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.7149 - accuracy: 0.3810 - val_loss: 0.6817 - val_accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.6978 - accuracy: 0.3810 - val_loss: 0.6745 - val_accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.6554 - accuracy: 0.3810 - val_loss: 0.6674 - val_accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.6801 - accuracy: 0.2857 - val_loss: 0.6608 - val_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.5948 - accuracy: 0.4286 - val_loss: 0.6539 - val_accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.6036 - accuracy: 0.4762 - val_loss: 0.6468 - val_accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.5945 - accuracy: 0.3810 - val_loss: 0.6401 - val_accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.5370 - accuracy: 0.4762 - val_loss: 0.6333 - val_accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5287 - accuracy: 0.3810 - val_loss: 0.6268 - val_accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5748 - accuracy: 0.4762 - val_loss: 0.6206 - val_accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5846 - accuracy: 0.5238 - val_loss: 0.6147 - val_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6730 - accuracy: 0.3810 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.6215 - accuracy: 0.2857 - val_loss: 0.6019 - val_accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.5202 - accuracy: 0.4286 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.6341 - accuracy: 0.2857 - val_loss: 0.5893 - val_accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.5104 - accuracy: 0.4286 - val_loss: 0.5830 - val_accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.5784 - accuracy: 0.4762 - val_loss: 0.5765 - val_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.6121 - accuracy: 0.3810 - val_loss: 0.5703 - val_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.4763 - accuracy: 0.4286 - val_loss: 0.5638 - val_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.5415 - accuracy: 0.2857 - val_loss: 0.5572 - val_accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.4053 - accuracy: 0.4762 - val_loss: 0.5503 - val_accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.4931 - accuracy: 0.4286 - val_loss: 0.5429 - val_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.4389 - accuracy: 0.3810 - val_loss: 0.5351 - val_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.5130 - accuracy: 0.4286 - val_loss: 0.5274 - val_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.3983 - accuracy: 0.4762 - val_loss: 0.5193 - val_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 0.4821 - accuracy: 0.2381 - val_loss: 0.5108 - val_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.3707 - accuracy: 0.4286 - val_loss: 0.5024 - val_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.3018 - accuracy: 0.4762 - val_loss: 0.4935 - val_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.4263 - accuracy: 0.3333 - val_loss: 0.4845 - val_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.3464 - accuracy: 0.4286 - val_loss: 0.4755 - val_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 0.4463 - accuracy: 0.3810 - val_loss: 0.4669 - val_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.4008 - accuracy: 0.4762 - val_loss: 0.4586 - val_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.2838 - accuracy: 0.3810 - val_loss: 0.4504 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.2059 - accuracy: 0.4762 - val_loss: 0.4416 - val_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.2663 - accuracy: 0.3333 - val_loss: 0.4322 - val_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.3396 - accuracy: 0.2857 - val_loss: 0.4232 - val_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.4145 - accuracy: 0.3810 - val_loss: 0.4152 - val_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 0.2445 - accuracy: 0.4286 - val_loss: 0.4068 - val_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.0771 - accuracy: 0.4286 - val_loss: 0.3979 - val_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1754 - accuracy: 0.2857 - val_loss: 0.3892 - val_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1172 - accuracy: 0.3810 - val_loss: 0.3803 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1278 - accuracy: 0.2857 - val_loss: 0.3721 - val_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 0.2508 - accuracy: 0.3810 - val_loss: 0.3646 - val_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1677 - accuracy: 0.3810 - val_loss: 0.3576 - val_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.0487 - accuracy: 0.4286 - val_loss: 0.3506 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.0261 - accuracy: 0.3810 - val_loss: 0.3427 - val_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 83ms/step - loss: 0.0241 - accuracy: 0.4762 - val_loss: 0.3346 - val_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.0658 - accuracy: 0.3810 - val_loss: 0.3264 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0368 - accuracy: 0.5926\n",
            "Loss: 0.036752503365278244, Accuracy: 0.5925925970077515\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Data Point 1: Predicted Risk = 0.67, Response = High Risk\n",
            "Data Point 2: Predicted Risk = 0.67, Response = High Risk\n",
            "Data Point 3: Predicted Risk = 0.44, Response = Low Risk\n",
            "Data Point 4: Predicted Risk = 0.45, Response = Low Risk\n",
            "Data Point 5: Predicted Risk = 0.45, Response = Low Risk\n",
            "Data Point 6: Predicted Risk = 0.82, Response = High Risk\n",
            "Data Point 7: Predicted Risk = 0.68, Response = High Risk\n",
            "Data Point 8: Predicted Risk = 0.42, Response = Low Risk\n",
            "Data Point 9: Predicted Risk = 0.69, Response = High Risk\n",
            "Data Point 10: Predicted Risk = 0.81, Response = High Risk\n",
            "Data Point 11: Predicted Risk = 0.67, Response = High Risk\n",
            "Data Point 12: Predicted Risk = 0.73, Response = High Risk\n",
            "Data Point 13: Predicted Risk = 0.43, Response = Low Risk\n",
            "Data Point 14: Predicted Risk = 0.82, Response = High Risk\n",
            "Data Point 15: Predicted Risk = 0.83, Response = High Risk\n",
            "Data Point 16: Predicted Risk = 0.64, Response = High Risk\n",
            "Data Point 17: Predicted Risk = 0.68, Response = High Risk\n",
            "Data Point 18: Predicted Risk = 0.29, Response = Low Risk\n",
            "Data Point 19: Predicted Risk = 0.83, Response = High Risk\n",
            "Data Point 20: Predicted Risk = 0.44, Response = Low Risk\n",
            "Data Point 21: Predicted Risk = 0.54, Response = High Risk\n",
            "Data Point 22: Predicted Risk = 0.79, Response = High Risk\n",
            "Data Point 23: Predicted Risk = 0.68, Response = High Risk\n",
            "Data Point 24: Predicted Risk = 0.80, Response = High Risk\n",
            "Data Point 25: Predicted Risk = 0.69, Response = High Risk\n",
            "Data Point 26: Predicted Risk = 0.69, Response = High Risk\n",
            "Data Point 27: Predicted Risk = 0.69, Response = High Risk\n"
          ]
        }
      ]
    }
  ]
}