{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3JSrYGuPlrV3Cvo1HRq/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/PIXEL-PIONEERS-TUTORIALS/blob/main/Vanishing_Gradients_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Gradient Monitoring Tool for GANs\n",
        "\n",
        "This code provides a PyTorch implementation of a gradient monitoring tool for Generative Adversarial Networks (GANs).\n",
        "The tool monitors the gradients of the generator and discriminator networks during training and provides recommendations\n",
        "for addressing vanishing gradients.\n",
        "\n",
        "Vanishing gradients occur when the gradients become extremely small, making it difficult for the model to learn and update\n",
        "its weights effectively. This can lead to slow convergence or even failure of the training process.\n",
        "\n",
        "The `GradientMonitor` class captures the gradients of each layer using backward hooks and analyzes them at the end of each\n",
        "epoch. If the average gradient of a layer falls below a specified threshold, it is considered a vanishing gradient, and\n",
        "corresponding recommendations are generated.\n",
        "\n",
        "The recommendations include suggestions such as adjusting the learning rate, modifying the architecture by adding skip\n",
        "connections or changing activation functions, applying gradient clipping, or using alternative loss functions.\n",
        "\n",
        "To use the `GradientMonitor`, create instances for both the generator and discriminator networks, register the backward\n",
        "hooks, and call the `monitor` method at the end of each training epoch.\n",
        "\n",
        "Note: This tool provides general recommendations based on common techniques for addressing vanishing gradients. The\n",
        "effectiveness of the recommendations may vary depending on the specific GAN architecture, dataset, and training setup.\n",
        "Experimentation and domain knowledge are crucial for successfully mitigating vanishing gradients.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jDvuQMtfDgun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wl2DfOdDYp_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class GradientMonitor:\n",
        "    def __init__(self, model, max_epochs, threshold=1e-4):\n",
        "        \"\"\"\n",
        "        Initialize the GradientMonitor.\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The model to monitor gradients for.\n",
        "            max_epochs (int): The maximum number of epochs for training.\n",
        "            threshold (float): The threshold for detecting vanishing gradients.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.max_epochs = max_epochs\n",
        "        self.threshold = threshold\n",
        "        self.gradients = {}\n",
        "        self.recommendations = []\n",
        "\n",
        "    def register_hooks(self):\n",
        "        \"\"\"\n",
        "        Register backward hooks to capture gradients of each layer during training.\n",
        "        \"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.register_hook(lambda grad, name=name: self.store_gradient(name, grad))\n",
        "\n",
        "    def store_gradient(self, name, grad):\n",
        "        \"\"\"\n",
        "        Store the gradient of each layer in the gradients dictionary.\n",
        "        \"\"\"\n",
        "        if name not in self.gradients:\n",
        "            self.gradients[name] = []\n",
        "        self.gradients[name].append(grad.clone().detach())\n",
        "\n",
        "    def analyze_gradients(self, epoch):\n",
        "        \"\"\"\n",
        "        Analyze the gradients to identify vanishing gradients and provide recommendations.\n",
        "        \"\"\"\n",
        "        vanishing_gradients = []\n",
        "        for name, grads in self.gradients.items():\n",
        "            avg_grad = torch.mean(torch.stack(grads))\n",
        "            if avg_grad.abs() < self.threshold:\n",
        "                vanishing_gradients.append(name)\n",
        "\n",
        "        if vanishing_gradients:\n",
        "            recommendation = f\"Epoch {epoch}: Vanishing gradients detected in layers: {', '.join(vanishing_gradients)}. \"\n",
        "            recommendation += \"Consider the following:\\n\"\n",
        "            recommendation += \"1. Adjust learning rate or use learning rate scheduling.\\n\"\n",
        "            recommendation += \"2. Modify the architecture by adding skip connections or changing activation functions.\\n\"\n",
        "            recommendation += \"3. Apply gradient clipping or use alternative loss functions.\\n\"\n",
        "            self.recommendations.append(recommendation)\n",
        "\n",
        "    def print_recommendations(self):\n",
        "        \"\"\"\n",
        "        Print the collected recommendations.\n",
        "        \"\"\"\n",
        "        if self.recommendations:\n",
        "            print(\"Recommendations:\")\n",
        "            for rec in self.recommendations:\n",
        "                print(rec)\n",
        "        else:\n",
        "            print(\"No vanishing gradients detected.\")\n",
        "\n",
        "    def monitor(self, epoch):\n",
        "        \"\"\"\n",
        "        Monitor the gradients at the end of each epoch.\n",
        "        \"\"\"\n",
        "        self.analyze_gradients(epoch)\n",
        "        if epoch == self.max_epochs - 1:\n",
        "            self.print_recommendations()\n",
        "\n",
        "        # Clear the stored gradients for the next epoch\n",
        "        self.gradients = {}\n",
        "\n",
        "# Example usage\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(100, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Create instances of the generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Create gradient monitors for generator and discriminator\n",
        "generator_monitor = GradientMonitor(generator, max_epochs=100)\n",
        "discriminator_monitor = GradientMonitor(discriminator, max_epochs=100)\n",
        "\n",
        "# Register the backward hooks\n",
        "generator_monitor.register_hooks()\n",
        "discriminator_monitor.register_hooks()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    # Train the GAN for one epoch\n",
        "    # ...\n",
        "\n",
        "    # Monitor gradients at the end of each epoch\n",
        "    generator_monitor.monitor(epoch)\n",
        "    discriminator_monitor.monitor(epoch)"
      ]
    }
  ]
}